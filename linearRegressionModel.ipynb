{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CiscoVirtualEducationAI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hGJgC1JjFIH",
        "outputId": "43971f88-d8a9-40ae-dd66-58c9c0402221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tf.__version__\n",
        "\n",
        "#imported dataset with pandas\n",
        "dataset = pd.read_excel('student-mat-edited.xlsx')\n",
        "x = dataset.iloc[:, :-1].values #variables\n",
        "y = dataset.iloc[:, -1].values #Grades output 1-20 scale\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "\n",
        "#splitting data sets into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "#initialize ANN as the sequential layers\n",
        "ann = tf.keras.models.Sequential()\n",
        "\n",
        "#adding the input layer and the first hidden layer\n",
        "ann.add(tf.keras.layers.Dense(units= 6, activation= 'relu'))\n",
        "\n",
        "#adding the second hidden layer\n",
        "ann.add(tf.keras.layers.Dense(units= 6, activation= 'relu'))\n",
        "\n",
        "#adding the output layer\n",
        "ann.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "#training the ANN\n",
        "#compiling the ANN \n",
        "#optimizer adam is a good optimizer for gradient descent\n",
        "#loss is the mean square error\n",
        "ann.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "#training the ANN model on the Training set\n",
        "ann.fit(x_train, y_train, batch_size = 32, epochs = 100)\n",
        "\n",
        "\n",
        "#predicting the results of the test set\n",
        "y_pred = ann.predict(x_test)\n",
        "np.set_printoptions(precision = 2)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2  2  0 ...  1  3  6]\n",
            " [ 1  2  0 ...  1  3  4]\n",
            " [ 1  2  3 ...  3  3 10]\n",
            " ...\n",
            " [ 1  1  3 ...  3  3  3]\n",
            " [ 3  1  0 ...  4  5  0]\n",
            " [ 1  1  0 ...  3  5  5]]\n",
            "[ 6  6 10 15 10 15 11  6 19 15  9 12 14 11 16 14 14 10  5 10 15 15 16 12\n",
            "  8  8 11 15 11 11 12 17 16 12 15  6 18 15 11 13 11 12 18 11  9  6 11 20\n",
            " 14  7 13 13 10 11 13 10 15 15  9 16 11 11  9  9 10 15 12  6  8 16 15 10\n",
            "  5 14 11 10 10 11 10  5 12 11  6 15 10  8  6 14 10  7  8 18  6 10 14 10\n",
            " 15 10 14  8  5 17 14  6 18 11  8 18 13 16 19 10 13 19  9 16 14 13  8 13\n",
            " 15 15 13 13  8 12 11  9  0 18  0  0 12 11  0  0  0  0 12 15  0  9 11 13\n",
            "  0 11  0 11  0 10  0 14 10  0 12  8 13 10 15 12  0  7  0 10  7 12 10 16\n",
            "  0 14  0 16 10  0  9  9 11  6  9 11  8 12 17  8 12 11 11 15  9 10 13  9\n",
            "  8 10 14 15 16 10 18 10 16 10 10  6 11  9  7 13 10  7  8 13 14  8 10 15\n",
            "  4  8  8 10  6  0 17 13 14  7 15 12  9 12 14 11  9 13  6 10 13 12 11  0\n",
            " 12 12  0 12  0 18 13  8  5 15  8 10  8  8 12  8 13 11 14  0 18  8 12  9\n",
            "  0 17 10 11 10  0  9 14 11 14 10 12  9  9  8 10  8 10 12 10 11 11 19 12\n",
            " 14 15 11 15 13 18 14 11  0  8 14 16 11 10 14 18 13 12 18  8 12 10  0 13\n",
            " 11 11 13 11  0  9 10 11 13  9 11 15 15 11 16 10  9 14  8 14  0  0  0 15\n",
            " 13  0 17 10 11  0 15  0 10 14 16  9 15 13  8 13  8  8 11  9 13 11 10 16\n",
            " 13 12 10 15 12 10 13  0 10 11  9 12 11  5 19 10 15 10 15 10 14  7 10  0\n",
            "  5 10  6  0  8  0  9 16  7 10  9]\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 128.8227\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 128.5441\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 128.1937\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 127.6919\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 126.8935\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 125.4447\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 122.6310\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 117.9305\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 110.6830\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 101.8577\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 91.1565\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 79.2117\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 66.3702\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 53.4140\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 41.6033\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 32.5644\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 26.8082\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 24.2243\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 23.5359\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 22.8995\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 22.5236\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 22.1900\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 21.9076\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 21.7559\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 21.5752\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 21.4340\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 21.3009\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 21.1128\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 21.0116\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 20.8474\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 20.7390\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 20.5740\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 20.5031\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 20.3400\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 20.2333\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 20.0806\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 19.9654\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 19.8379\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 19.7145\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 19.5997\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 19.4985\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 19.3854\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 19.2797\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 19.1558\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 19.0997\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 19.0279\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 18.8764\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 18.7745\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 18.6577\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 18.5821\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 18.5312\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 18.3939\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 18.3357\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 18.2283\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 18.1620\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 18.0879\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.9956\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.9895\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.8766\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.7900\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.7133\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.6447\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.5884\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.5338\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 17.4341\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.3910\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 17.3184\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 17.2538\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 17.2098\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.1485\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.1094\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 17.0424\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.9721\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.9075\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 16.9471\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 16.8232\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.7705\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.7306\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.6897\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.6360\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.5860\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.5607\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.4894\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 16.4440\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.4012\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.3584\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.3107\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.3172\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.2609\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.2210\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.1769\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 16.1387\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.0897\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.0846\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 16.0450\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 15.9968\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 15.9716\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 15.9476\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 15.9008\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 15.8898\n",
            "[[12.68 14.  ]\n",
            " [10.17 10.  ]\n",
            " [12.41  9.  ]\n",
            " [12.66 15.  ]\n",
            " [10.43 16.  ]\n",
            " [12.   12.  ]\n",
            " [ 6.22 14.  ]\n",
            " [10.36 11.  ]\n",
            " [ 8.69  9.  ]\n",
            " [ 7.72 15.  ]\n",
            " [12.27 10.  ]\n",
            " [11.9  11.  ]\n",
            " [12.26 10.  ]\n",
            " [12.1   8.  ]\n",
            " [12.11 15.  ]\n",
            " [11.35 14.  ]\n",
            " [10.94 12.  ]\n",
            " [ 9.95  0.  ]\n",
            " [14.11 10.  ]\n",
            " [ 7.85 16.  ]\n",
            " [ 5.91  0.  ]\n",
            " [ 0.19  0.  ]\n",
            " [11.06  8.  ]\n",
            " [10.87 16.  ]\n",
            " [12.7  12.  ]\n",
            " [10.93 15.  ]\n",
            " [11.34 10.  ]\n",
            " [10.09 19.  ]\n",
            " [ 9.96  0.  ]\n",
            " [11.28 14.  ]\n",
            " [ 3.2   0.  ]\n",
            " [ 7.37 10.  ]\n",
            " [15.39  8.  ]\n",
            " [ 9.27  4.  ]\n",
            " [11.01 13.  ]\n",
            " [10.7  10.  ]\n",
            " [10.67 13.  ]\n",
            " [ 7.1  18.  ]\n",
            " [ 7.8   8.  ]\n",
            " [ 8.94 11.  ]\n",
            " [11.97  6.  ]\n",
            " [10.17 18.  ]\n",
            " [ 8.73  8.  ]\n",
            " [ 8.75  8.  ]\n",
            " [14.37 11.  ]\n",
            " [ 3.45  0.  ]\n",
            " [10.41 13.  ]\n",
            " [ 6.83  0.  ]\n",
            " [ 9.86 13.  ]\n",
            " [ 2.41  7.  ]\n",
            " [13.26 10.  ]\n",
            " [13.29 18.  ]\n",
            " [12.28 10.  ]\n",
            " [ 7.96  9.  ]\n",
            " [10.14 11.  ]\n",
            " [11.32  6.  ]\n",
            " [12.92  9.  ]\n",
            " [ 5.43 13.  ]\n",
            " [ 9.93 15.  ]\n",
            " [ 8.66 14.  ]\n",
            " [13.83 14.  ]\n",
            " [12.34 15.  ]\n",
            " [10.94 14.  ]\n",
            " [ 9.68 15.  ]\n",
            " [12.36  0.  ]\n",
            " [13.35 15.  ]\n",
            " [12.36 19.  ]\n",
            " [ 9.27 12.  ]\n",
            " [ 8.7   6.  ]\n",
            " [14.94  9.  ]\n",
            " [ 9.95  8.  ]\n",
            " [10.98 15.  ]\n",
            " [11.51  5.  ]\n",
            " [ 1.07  0.  ]\n",
            " [11.53  9.  ]\n",
            " [ 6.87  0.  ]\n",
            " [11.43 11.  ]\n",
            " [11.41 18.  ]\n",
            " [ 2.18  0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}